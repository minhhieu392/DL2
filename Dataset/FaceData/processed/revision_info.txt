arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
tensorflow version: 2.7.0-dev20210629
--------------------
git hash: b'62b1e0c7a4ede2deb8388de8933d08a7f487b159'
--------------------
b'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\nindex 7f98ca7..e38ee2e 100644\n--- a/src/align/detect_face.py\n+++ b/src/align/detect_face.py\n@@ -82,13 +82,13 @@ class Network(object):\n         session: The current TensorFlow session\n         ignore_missing: If true, serialized weights for missing layers are ignored.\n         """\n-        data_dict = np.load(data_path, encoding=\'latin1\').item() #pylint: disable=no-member\n+        data_dict = np.load(data_path, encoding=\'latin1\',allow_pickle=True).item() #pylint: disable=no-member\n \n         for op_name in data_dict:\n-            with tf.variable_scope(op_name, reuse=True):\n+            with tf.compat.v1.variable_scope(op_name, reuse=True):\n                 for param_name, data in iteritems(data_dict[op_name]):\n                     try:\n-                        var = tf.get_variable(param_name)\n+                        var = tf.compat.v1.get_variable(param_name)\n                         session.run(var.assign(data))\n                     except ValueError:\n                         if not ignore_missing:\n@@ -122,7 +122,7 @@ class Network(object):\n \n     def make_var(self, name, shape):\n         """Creates a new TensorFlow variable."""\n-        return tf.get_variable(name, shape, trainable=self.trainable)\n+        return tf.compat.v1.get_variable(name, shape, trainable=self.trainable)\n \n     def validate_padding(self, padding):\n         """Verifies that the padding is one of the supported ones."""\n@@ -149,32 +149,32 @@ class Network(object):\n         assert c_i % group == 0\n         assert c_o % group == 0\n         # Convolution for a given input and kernel\n-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n-        with tf.variable_scope(name) as scope:\n+        convolve = lambda i, k: tf.compat.v1.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n+        with tf.compat.v1.variable_scope(name) as scope:\n             kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n             # This is the common-case. Convolve the input without any further complications.\n             output = convolve(inp, kernel)\n             # Add the biases\n             if biased:\n                 biases = self.make_var(\'biases\', [c_o])\n-                output = tf.nn.bias_add(output, biases)\n+                output = tf.compat.v1.nn.bias_add(output, biases)\n             if relu:\n                 # ReLU non-linearity\n-                output = tf.nn.relu(output, name=scope.name)\n+                output = tf.compat.v1.nn.relu(output, name=scope.name)\n             return output\n \n     @layer\n     def prelu(self, inp, name):\n-        with tf.variable_scope(name):\n+        with tf.compat.v1.variable_scope(name):\n             i = int(inp.get_shape()[-1])\n             alpha = self.make_var(\'alpha\', shape=(i,))\n-            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n+            output = tf.compat.v1.nn.relu(inp) + tf.compat.v1.multiply(alpha, -tf.compat.v1.nn.relu(-inp))\n         return output\n \n     @layer\n     def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n         self.validate_padding(padding)\n-        return tf.nn.max_pool(inp,\n+        return tf.compat.v1.nn.max_pool(inp,\n                               ksize=[1, k_h, k_w, 1],\n                               strides=[1, s_h, s_w, 1],\n                               padding=padding,\n@@ -182,19 +182,19 @@ class Network(object):\n \n     @layer\n     def fc(self, inp, num_out, name, relu=True):\n-        with tf.variable_scope(name):\n+        with tf.compat.v1.variable_scope(name):\n             input_shape = inp.get_shape()\n             if input_shape.ndims == 4:\n                 # The input is spatial. Vectorize it first.\n                 dim = 1\n                 for d in input_shape[1:].as_list():\n                     dim *= int(d)\n-                feed_in = tf.reshape(inp, [-1, dim])\n+                feed_in = tf.compat.v1.reshape(inp, [-1, dim])\n             else:\n-                feed_in, dim = (inp, input_shape[-1].value)\n+                feed_in, dim = (inp, input_shape[-1])\n             weights = self.make_var(\'weights\', shape=[dim, num_out])\n             biases = self.make_var(\'biases\', [num_out])\n-            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n+            op = tf.compat.v1.nn.relu_layer if relu else tf.compat.v1.nn.xw_plus_b\n             fc = op(feed_in, weights, biases, name=name)\n             return fc\n \n@@ -207,10 +207,10 @@ class Network(object):\n     """\n     @layer\n     def softmax(self, target, axis, name=None):\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\n-        target_exp = tf.exp(target-max_axis)\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n-        softmax = tf.div(target_exp, normalize, name)\n+        max_axis = tf.compat.v1.reduce_max(target, axis, keepdims=True)\n+        target_exp = tf.compat.v1.exp(target-max_axis)\n+        normalize = tf.compat.v1.reduce_sum(target_exp, axis, keepdims=True)\n+        softmax = tf.compat.v1.div(target_exp, normalize, name)\n         return softmax\n     \n class PNet(Network):\n@@ -277,16 +277,16 @@ def create_mtcnn(sess, model_path):\n     if not model_path:\n         model_path,_ = os.path.split(os.path.realpath(__file__))\n \n-    with tf.variable_scope(\'pnet\'):\n-        data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n+    with tf.compat.v1.variable_scope(\'pnet\'):\n+        data = tf.compat.v1.placeholder(tf.float32, (None,None,None,3), \'input\')\n         pnet = PNet({\'data\':data})\n         pnet.load(os.path.join(model_path, \'det1.npy\'), sess)\n-    with tf.variable_scope(\'rnet\'):\n-        data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n+    with tf.compat.v1.variable_scope(\'rnet\'):\n+        data = tf.compat.v1.placeholder(tf.compat.v1.float32, (None,24,24,3), \'input\')\n         rnet = RNet({\'data\':data})\n         rnet.load(os.path.join(model_path, \'det2.npy\'), sess)\n-    with tf.variable_scope(\'onet\'):\n-        data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n+    with tf.compat.v1.variable_scope(\'onet\'):\n+        data = tf.compat.v1.placeholder(tf.compat.v1.float32, (None,48,48,3), \'input\')\n         onet = ONet({\'data\':data})\n         onet.load(os.path.join(model_path, \'det3.npy\'), sess)\n         \ndiff --git a/src/align_dataset_mtcnn.py b/src/align_dataset_mtcnn.py\nindex 7d5e735..f892ac1 100644\n--- a/src/align_dataset_mtcnn.py\n+++ b/src/align_dataset_mtcnn.py\n@@ -25,7 +25,11 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-from scipy import misc\n+from scipy import *\n+from skimage.transform import resize\n+\n+import imageio\n+#from scipy.misc import imread\n import sys\n import os\n import argparse\n@@ -49,8 +53,8 @@ def main(args):\n     print(\'Creating networks and loading parameters\')\n     \n     with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n         with sess.as_default():\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n     \n@@ -80,7 +84,8 @@ def main(args):\n                 print(image_path)\n                 if not os.path.exists(output_filename):\n                     try:\n-                        img = misc.imread(image_path)\n+                        #img = misc.imread(image_path)\n+                        img = imageio.imread(image_path)\n                     except (IOError, ValueError, IndexError) as e:\n                         errorMessage = \'{}: {}\'.format(image_path, e)\n                         print(errorMessage)\n@@ -121,14 +126,14 @@ def main(args):\n                                 bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\n                                 bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n                                 cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n+                                scaled = resize(cropped, (args.image_size, args.image_size))\n                                 nrof_successfully_aligned += 1\n                                 filename_base, file_extension = os.path.splitext(output_filename)\n                                 if args.detect_multiple_faces:\n                                     output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\n                                 else:\n                                     output_filename_n = "{}{}".format(filename_base, file_extension)\n-                                misc.imsave(output_filename_n, scaled)\n+                                imageio.imsave(output_filename_n, scaled)\n                                 text_file.write(\'%s %d %d %d %d\\n\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\n                         else:\n                             print(\'Unable to align "%s"\' % image_path)\ndiff --git a/src/classifier.py b/src/classifier.py\nindex e7189bc..94d627e 100644\n--- a/src/classifier.py\n+++ b/src/classifier.py\n@@ -38,9 +38,9 @@ from sklearn.svm import SVC\n \n def main(args):\n   \n-    with tf.Graph().as_default():\n+    with tf.compat.v1.Graph().as_default():\n       \n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n             \n             np.random.seed(seed=args.seed)\n             \n@@ -69,9 +69,9 @@ def main(args):\n             facenet.load_model(args.model)\n             \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n             embedding_size = embeddings.get_shape()[1]\n             \n             # Run forward pass to calculate embeddings\ndiff --git a/src/face_rec.py b/src/face_rec.py\nindex 1eb578d..5fde69a 100644\n--- a/src/face_rec.py\n+++ b/src/face_rec.py\n@@ -36,11 +36,11 @@ def main():\n         model, class_names = pickle.load(file)\n     print("Custom Classifier, Successfully loaded")\n \n-    with tf.Graph().as_default():\n+    with tf.compat.v1.Graph().as_default():\n \n         # Cai dat GPU neu co\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n \n         with sess.as_default():\n \n@@ -49,9 +49,9 @@ def main():\n             facenet.load_model(FACENET_MODEL_PATH)\n \n             # Lay tensor input va output\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n             embedding_size = embeddings.get_shape()[1]\n \n             # Cai dat cac mang con\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\nindex cfbd4f4..ef93425 100644\n--- a/src/face_rec_cam.py\n+++ b/src/face_rec_cam.py\n@@ -39,10 +39,10 @@ def main():\n         model, class_names = pickle.load(file)\n     print("Custom Classifier, Successfully loaded")\n \n-    with tf.Graph().as_default():\n+    with tf.compat.v1.Graph().as_default():\n \n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n \n         with sess.as_default():\n \n@@ -51,9 +51,9 @@ def main():\n             facenet.load_model(FACENET_MODEL_PATH)\n \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n             embedding_size = embeddings.get_shape()[1]\n \n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\ndiff --git a/src/facenet.py b/src/facenet.py\nindex bfe6802..9f7fbd2 100644\n--- a/src/facenet.py\n+++ b/src/facenet.py\n@@ -31,7 +31,9 @@ import os\n from subprocess import Popen, PIPE\n import tensorflow as tf\n import numpy as np\n-from scipy import misc\n+from scipy import *\n+import imageio\n+from skimage.transform import resize\n from sklearn.model_selection import KFold\n from scipy import interpolate\n from tensorflow.python.training import training\n@@ -52,12 +54,12 @@ def triplet_loss(anchor, positive, negative, alpha):\n     Returns:\n       the triplet loss according to the FaceNet paper as a float tensor.\n     """\n-    with tf.variable_scope(\'triplet_loss\'):\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n+    with tf.compat.v1.variable_scope(\'triplet_loss\'):\n+        pos_dist = tf.compat.v1.reduce_sum(tf.compat.v1.square(tf.compat.v1.subtract(anchor, positive)), 1)\n+        neg_dist = tf.compat.v1.reduce_sum(tf.compat.v1.square(tf.compat.v1.subtract(anchor, negative)), 1)\n         \n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n+        basic_loss = tf.compat.v1.add(tf.compat.v1.subtract(pos_dist,neg_dist), alpha)\n+        loss = tf.compat.v1.reduce_mean(tf.compat.v1.maximum(basic_loss, 0.0), 0)\n       \n     return loss\n   \n@@ -66,14 +68,14 @@ def center_loss(features, label, alfa, nrof_classes):\n        (http://ydwen.github.io/papers/WenECCV16.pdf)\n     """\n     nrof_features = features.get_shape()[1]\n-    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n-    label = tf.reshape(label, [-1])\n-    centers_batch = tf.gather(centers, label)\n+    centers = tf.compat.v1.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n+        initializer=tf.compat.v1.constant_initializer(0), trainable=False)\n+    label = tf.compat.v1.reshape(label, [-1])\n+    centers_batch = tf.compat.v1.gather(centers, label)\n     diff = (1 - alfa) * (centers_batch - features)\n-    centers = tf.scatter_sub(centers, label, diff)\n-    with tf.control_dependencies([centers]):\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\n+    centers = tf.compat.v1.scatter_sub(centers, label, diff)\n+    with tf.compat.v1.control_dependencies([centers]):\n+        loss = tf.compat.v1.reduce_mean(tf.compat.v1.square(features - centers_batch))\n     return loss, centers\n \n def get_image_paths_and_labels(dataset):\n@@ -92,7 +94,7 @@ def shuffle_examples(image_paths, labels):\n \n def random_rotate_image(image):\n     angle = np.random.uniform(low=-10.0, high=10.0)\n-    return misc.imrotate(image, angle, \'bicubic\')\n+    return imageio.imrotate(image, angle, \'bicubic\')\n   \n # 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n RANDOM_ROTATE = 1\n@@ -105,30 +107,30 @@ def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batc\n     for _ in range(nrof_preprocess_threads):\n         filenames, label, control = input_queue.dequeue()\n         images = []\n-        for filename in tf.unstack(filenames):\n-            file_contents = tf.read_file(filename)\n-            image = tf.image.decode_image(file_contents, 3)\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n+        for filename in tf.compat.v1.unstack(filenames):\n+            file_contents = tf.compat.v1.read_file(filename)\n+            image = tf.compat.v1.image.decode_image(file_contents, 3)\n+            image = tf.compat.v1.cond(get_control_flag(control[0], RANDOM_ROTATE),\n+                            lambda:tf.compat.v1.py_func(random_rotate_image, [image], tf.uint8), \n+                            lambda:tf.compat.v1.identity(image))\n+            image = tf.compat.v1.cond(get_control_flag(control[0], RANDOM_CROP), \n+                            lambda:tf.compat.v1.random_crop(image, image_size + (3,)), \n+                            lambda:tf.compat.v1.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n             image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n-            image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n+                            lambda:tf.compat.v1.image.random_flip_left_right(image),\n+                            lambda:tf.compat.v1.identity(image))\n+            image = tf.compat.v1.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n+                            lambda:(tf.compat.v1.cast(image, tf.float32) - 127.5)/128.0,\n+                            lambda:tf.compat.v1.image.per_image_standardization(image))\n+            image = tf.compat.v1.cond(get_control_flag(control[0], FLIP),\n+                            lambda:tf.compat.v1.image.flip_left_right(image),\n+                            lambda:tf.compat.v1.identity(image))\n             #pylint: disable=no-member\n             image.set_shape(image_size + (3,))\n             images.append(image)\n         images_and_labels_list.append([images, label])\n \n-    image_batch, label_batch = tf.train.batch_join(\n+    image_batch, label_batch = tf.compat.v1.train.batch_join(\n         images_and_labels_list, batch_size=batch_size_placeholder, \n         shapes=[image_size + (3,), ()], enqueue_many=True,\n         capacity=4 * nrof_preprocess_threads * 100,\n@@ -151,8 +153,8 @@ def _add_loss_summaries(total_loss):\n       loss_averages_op: op for generating moving averages of losses.\n     """\n     # Compute the moving average of all individual losses and the total loss.\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n-    losses = tf.get_collection(\'losses\')\n+    loss_averages = tf.compat.v1.train.ExponentialMovingAverage(0.9, name=\'avg\')\n+    losses = tf.compat.v1.get_collection(\'losses\')\n     loss_averages_op = loss_averages.apply(losses + [total_loss])\n   \n     # Attach a scalar summmary to all individual losses and the total loss; do the\n@@ -160,8 +162,8 @@ def _add_loss_summaries(total_loss):\n     for l in losses + [total_loss]:\n         # Name each loss as \'(raw)\' and name the moving average version of the loss\n         # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\n+        tf.compat.v1.summary.scalar(l.op.name +\' (raw)\', l)\n+        tf.compat.v1.summary.scalar(l.op.name, loss_averages.average(l))\n   \n     return loss_averages_op\n \n@@ -170,17 +172,17 @@ def train(total_loss, global_step, optimizer, learning_rate, moving_average_deca\n     loss_averages_op = _add_loss_summaries(total_loss)\n \n     # Compute gradients.\n-    with tf.control_dependencies([loss_averages_op]):\n+    with tf.compat.v1.control_dependencies([loss_averages_op]):\n         if optimizer==\'ADAGRAD\':\n-            opt = tf.train.AdagradOptimizer(learning_rate)\n+            opt = tf.compat.v1.train.AdagradOptimizer(learning_rate)\n         elif optimizer==\'ADADELTA\':\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n+            opt = tf.compat.v1.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n         elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n+            opt = tf.compat.v1.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n         elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n+            opt = tf.compat.v1.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n         elif optimizer==\'MOM\':\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n+            opt = tf.compat.v1.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n         else:\n             raise ValueError(\'Invalid optimization algorithm\')\n     \n@@ -191,22 +193,22 @@ def train(total_loss, global_step, optimizer, learning_rate, moving_average_deca\n   \n     # Add histograms for trainable variables.\n     if log_histograms:\n-        for var in tf.trainable_variables():\n-            tf.summary.histogram(var.op.name, var)\n+        for var in tf.compat.v1.trainable_variables():\n+            tf.compat.v1.summary.histogram(var.op.name, var)\n    \n     # Add histograms for gradients.\n     if log_histograms:\n         for grad, var in grads:\n             if grad is not None:\n-                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n+                tf.compat.v1.summary.histogram(var.op.name + \'/gradients\', grad)\n   \n     # Track the moving averages of all trainable variables.\n-    variable_averages = tf.train.ExponentialMovingAverage(\n+    variable_averages = tf.compat.v1.train.ExponentialMovingAverage(\n         moving_average_decay, global_step)\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n+    variables_averages_op = variable_averages.apply(tf.compat.v1.trainable_variables())\n   \n-    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n-        train_op = tf.no_op(name=\'train\')\n+    with tf.compat.v1.control_dependencies([apply_gradient_op, variables_averages_op]):\n+        train_op = tf.compat.v1.no_op(name=\'train\')\n   \n     return train_op\n \n@@ -244,7 +246,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\n     nrof_samples = len(image_paths)\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\n     for i in range(nrof_samples):\n-        img = misc.imread(image_paths[i])\n+        img = imageio.imread(image_paths[i])\n         if img.ndim == 2:\n             img = to_rgb(img)\n         if do_prewhiten:\n@@ -368,9 +370,9 @@ def load_model(model, input_map=None):\n     if (os.path.isfile(model_exp)):\n         print(\'Model filename: %s\' % model_exp)\n         with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n+            graph_def = tf.compat.v1.GraphDef()\n             graph_def.ParseFromString(f.read())\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n+            tf.compat.v1.import_graph_def(graph_def, input_map=input_map, name=\'\')\n     else:\n         print(\'Model directory: %s\' % model_exp)\n         meta_file, ckpt_file = get_model_filenames(model_exp)\n@@ -378,8 +380,8 @@ def load_model(model, input_map=None):\n         print(\'Metagraph file: %s\' % meta_file)\n         print(\'Checkpoint file: %s\' % ckpt_file)\n       \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n+        saver = tf.compat.v1.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n+        saver.restore(tf.compat.v1.get_default_session(), os.path.join(model_exp, ckpt_file))\n     \n def get_model_filenames(model_dir):\n     files = os.listdir(model_dir)\n@@ -389,7 +391,7 @@ def get_model_filenames(model_dir):\n     elif len(meta_files)>1:\n         raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n     meta_file = meta_files[0]\n-    ckpt = tf.train.get_checkpoint_state(model_dir)\n+    ckpt = tf.compat.v1.train.get_checkpoint_state(model_dir)\n     if ckpt and ckpt.model_checkpoint_path:\n         ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n         return meta_file, ckpt_file\n@@ -538,7 +540,7 @@ def store_revision_info(src_path, output_dir, arg_string):\n     rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n     with open(rev_info_filename, "w") as text_file:\n         text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n-        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n+        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.compat.v1.__version__)  # @UndefinedVariable\n         text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n         text_file.write(\'%s\' % git_diff)\n \ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\ndeleted file mode 100644\nindex a503c89..0000000\nBinary files a/video/camtest.mp4 and /dev/null differ'